{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b0fe8b-2363-4522-a1da-b839b41c6150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e363b80e-650e-447c-9caa-9ead00a47466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset, random_split\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import transforms as T\n",
    "from torchvision import models, datasets\n",
    "from random import randint\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122f0a3d-dc6d-419b-856a-24d448f7fe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6168e29f-4e0d-479c-963d-201ef6f864c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'tiny-imagenet-200' # Original images come in shapes of [3,64,64]\n",
    "\n",
    "# Define training and validation data paths\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train') \n",
    "TEST_DIR = os.path.join(DATA_DIR, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189bea63-78f9-49fb-8912-78c60e144f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "def show_batch(dataloader):\n",
    "    dataiter = iter(dataloader)\n",
    "    images, labels = dataiter.next()    \n",
    "    imshow(make_grid(images)) # Using Torchvision.utils make_grid function\n",
    "    \n",
    "def show_image(dataloader):\n",
    "    dataiter = iter(dataloader)\n",
    "    images, labels = dataiter.next()\n",
    "    random_num = randint(0, len(images)-1)\n",
    "    imshow(images[random_num])\n",
    "    label = labels[random_num]\n",
    "    print(f'Label: {label}, Shape: {images[random_num].shape}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e52dbe-ac9d-419d-af98-658e8c2f102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup function to create dataloaders for image datasets\n",
    "def generate_dataloader(data, name, transform, batch_size=32, val_split=None):\n",
    "    if data is None: \n",
    "        return None\n",
    "    \n",
    "    # Read image files to pytorch dataset using ImageFolder, a generic data \n",
    "    # loader where images are in format root/label/filename\n",
    "    # See https://pytorch.org/vision/stable/datasets.html\n",
    "    if transform is None:\n",
    "        dataset = datasets.ImageFolder(data, transform=T.ToTensor())\n",
    "    else:\n",
    "        dataset = datasets.ImageFolder(data, transform=transform)\n",
    "\n",
    "    if val_split:\n",
    "        val_ds_length = int(len(dataset)*val_split)\n",
    "        train_ds_length = len(dataset) - val_ds_length\n",
    "        train_split_dataset, val_split_dataset = random_split(dataset, [train_ds_length, val_ds_length], generator=torch.Generator().manual_seed(42))\n",
    "        print(f'Number of training samples: {len(train_split_dataset)}, Number of unique train labels: {len(set(train_split_dataset.dataset.targets))}')\n",
    "        print(f'Number of validation samples: {len(val_split_dataset)}, Number of unique validation labels: {len(set(val_split_dataset.dataset.targets))}')\n",
    "\n",
    "    # Set options for device\n",
    "    if use_cuda:\n",
    "        kwargs = {\"pin_memory\": True, \"num_workers\": 1}\n",
    "    else:\n",
    "        kwargs = {}\n",
    "    \n",
    "    if not val_split:\n",
    "      # Wrap image dataset (defined above) in dataloader \n",
    "      dataloader = DataLoader(dataset, batch_size=batch_size, \n",
    "                          shuffle=(name==\"train\"), \n",
    "                          **kwargs)\n",
    "      \n",
    "      return dataloader\n",
    "    else:\n",
    "        train_dataloader = DataLoader(train_split_dataset, batch_size=batch_size, \n",
    "                          shuffle=(name==\"train\"), \n",
    "                          **kwargs)\n",
    "        val_dataloader = DataLoader(val_split_dataset, batch_size=batch_size, \n",
    "                          shuffle=(name==\"train\"), \n",
    "                          **kwargs)\n",
    "        return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04457ff3-6354-441b-9204-bd3afdc57917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate validation subfolders for the validation images based on\n",
    "# their labels indicated in the val_annotations txt file\n",
    "test_img_dir = os.path.join(TEST_DIR, 'images')\n",
    "\n",
    "# Open and read val annotations text file\n",
    "fp = open(os.path.join(TEST_DIR, 'val_annotations.txt'), 'r')\n",
    "data = fp.readlines()\n",
    "\n",
    "# Create dictionary to store img filename (word 0) and corresponding\n",
    "# label (word 1) for every line in the txt file (as key value pair)\n",
    "val_img_dict = {}\n",
    "for line in data:\n",
    "    words = line.split('\\t')\n",
    "    val_img_dict[words[0]] = words[1]\n",
    "fp.close()\n",
    "\n",
    "# Display first 10 entries of resulting val_img_dict dictionary\n",
    "print({k: val_img_dict[k] for k in list(val_img_dict)[:10]})\n",
    "\n",
    "# Create subfolders (if not present) for validation images based on label,\n",
    "# and move images into the respective folders\n",
    "for img, folder in val_img_dict.items():\n",
    "    newpath = (os.path.join(test_img_dir, folder))\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    if os.path.exists(os.path.join(test_img_dir, img)):\n",
    "        os.rename(os.path.join(test_img_dir, img), os.path.join(newpath, img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769d14cf-64b5-4a4f-b1bb-f3cbb837d185",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.4802369434455423, 0.4480670369274663, 0.39750364210899203]\n",
    "std = [0.2764364260576532, 0.268863276261085, 0.28158992530131444]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0163cf-e6a3-4733-a71c-26ba67b95b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation - optional depending on future resnet implementation\n",
    "\n",
    "# Define transformation sequence for image pre-processing\n",
    "# If not using pre-trained model, normalize with 0.5, 0.5, 0.5 (mean and SD)\n",
    "# If using pre-trained ImageNet, normalize with mean=[0.485, 0.456, 0.406], \n",
    "# std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    # T.Resize(32), # Resize images to 256 x 256\n",
    "    # T.CenterCrop(224), # Center crop image\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomRotation(15),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "test_transform = T.Compose([\n",
    "                # T.Resize(32), # Resize images to 256 x 256\n",
    "                # T.CenterCrop(224), # Center crop image\n",
    "                # T.RandomHorizontalFlip(),\n",
    "                T.ToTensor(),  # Converting cropped images to tensors\n",
    "                T.Normalize(mean, std)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9b9975-d028-4ea7-a724-56f4e38bfb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f167fe-05f3-4a10-8818-0e7b71bbf765",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL.Image import NONE\n",
    "#\n",
    "# Define batch size for DataLoaders\n",
    "batch_size = 128\n",
    "\n",
    "# Create DataLoaders for pre-trained models (normalized based on specific requirements)\n",
    "train_loader = generate_dataloader(TRAIN_DIR, \"train\",\n",
    "                                  transform=train_transform, batch_size=batch_size)\n",
    "\n",
    "test_loader = generate_dataloader(test_img_dir, \"val\",\n",
    "                                 transform=test_transform, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc9f761-fafd-4eb6-9674-f50b9d37ae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa29c1a0-b6ef-4436-81b2-873fbc43f09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "    \n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Input size: [batch, 3, 32, 32]\n",
    "        # Output size: [batch, 3, 32, 32]\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 12, 4, stride=2, padding=1),            # [batch, 12, 16, 16]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(12, 24, 4, stride=2, padding=1),           # [batch, 24, 8, 8]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(24, 48, 4, stride=2, padding=1),           # [batch, 48, 4, 4]\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(48, 24, 4, stride=2, padding=1),  # [batch, 24, 8, 8]\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(24, 12, 4, stride=2, padding=1),  # [batch, 12, 16, 16]\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(12, 3, 4, stride=2, padding=1),   # [batch, 3, 32, 32]\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded, encoded.view(-1, 48*8*8)\n",
    "\n",
    "def test():\n",
    "    net = Autoencoder()\n",
    "    net = net.to(device)\n",
    "    item = iter(train_loader)\n",
    "    img, _ = next(item)\n",
    "\n",
    "    img = img[0]\n",
    "    img = img[None, :]\n",
    "    img = img.to(device)\n",
    "    img_cons, rep  = net(img)\n",
    "    std_t = torch.tensor(std).to(device)\n",
    "    mean_t = torch.tensor(mean).to(device)\n",
    "    img_cons = img_cons * std_t[:, None, None] + mean_t[:, None, None]\n",
    "    img = img * std_t[:, None, None] + mean_t[:, None, None]\n",
    "    # print(rep)\n",
    "    print(rep.shape, \"  \", img_cons.shape)\n",
    "    temp_r = imshow(img.cpu()[0])\n",
    "    imshow(img_cons.detach().cpu()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cc83f1-ec5f-438d-8d6c-9b41e1a1e77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3574fa37-5e23-4c88-8d5f-51d5889b1c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    'train': train_loader,\n",
    "    'val': test_loader,\n",
    "    'test': test_loader\n",
    "}\n",
    "dataset_sizes = {\n",
    "    'train': len(train_loader.dataset),\n",
    "    'val': len(test_loader.dataset),\n",
    "    'test': len(test_loader.dataset),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d9c748-9de0-4243-8dc1-30ce0f9e1243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "from torch import autograd\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train','val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, _ in tqdm(dataloaders[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    reconstructed_img, hidden_rep = model(inputs)\n",
    "                    \n",
    "                    # l1_loss = sparse_loss(model, inputs) * 0.001\n",
    "                    loss = criterion(reconstructed_img, inputs)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        with autograd.detect_anomaly():\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss}')          \n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "#             if phase == 'val':\n",
    "                print('Saving..')\n",
    "                state = {\n",
    "                    'model': model.state_dict(),\n",
    "                    'loss': epoch_loss,\n",
    "                    'epoch': epoch,\n",
    "                }\n",
    "                # if not os.path.isdir('checkpoint'):\n",
    "                #     os.mkdir('checkpoint')\n",
    "                torch.save(state, './ae_tiny_imagenet.pth')\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val loss: {best_loss:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fdb1c2-c67d-48e6-a076-a8ee39463099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "model_ft = Autoencoder()\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "exp_lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer_ft, milestones=[60, 120, 160], gamma=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0823ebb3-4b10-4d2d-9f5a-14f4a994ff5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d0c3f1-00a1-4994-84cf-ce38a581bc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "device = 'cuda'\n",
    "model_ft.eval()\n",
    "item = iter(train_loader)\n",
    "img, _ = next(item)\n",
    "\n",
    "img = img[0]\n",
    "img = img[None, :]\n",
    "img = img.to(device)\n",
    "img_cons, rep  = model_ft(img)\n",
    "std_t = torch.tensor(std).to(device)\n",
    "mean_t = torch.tensor(mean).to(device)\n",
    "img_cons = img_cons * std_t[:, None, None] + mean_t[:, None, None]\n",
    "img = img * std_t[:, None, None] + mean_t[:, None, None]\n",
    "print(rep.shape, \"  \", img_cons.shape)\n",
    "temp_r = imshow(img.cpu()[0])\n",
    "imshow(img_cons.detach().cpu()[0])\n",
    "print(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ca8aa4-f69c-470c-b10c-c8cc33a5d2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_embeddings = []\n",
    "train_image_labels = []\n",
    "\n",
    "def populate_embedding_lable_list(model):\n",
    "    global train_image_embeddings, train_image_labels\n",
    "\n",
    "    model.eval()   # Set model to evaluate mode\n",
    "    # Iterate over data.\n",
    "    for inputs, labels in tqdm(dataloaders['train']):\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        # forward\n",
    "        # track history if only in train\n",
    "        with torch.set_grad_enabled(False):\n",
    "            reconstructed_img, hidden_rep = model(inputs)\n",
    "            train_image_embeddings.extend(hidden_rep)\n",
    "            train_image_labels.extend(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aee2267-89cc-4593-af62-896aabf3d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "populate_embedding_lable_list(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f899ab1e-ae2d-44e5-a908-4194afda8bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_labels = torch.flatten(torch.tensor(train_image_labels))\n",
    "\n",
    "train_image_embeddings_tensor = torch.stack(train_image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e04b0bb-cf75-42af-b11c-34621aed5222",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sorted(set(list(train_loader.dataset.targets)))))\n",
    "print(len(sorted(set(list(train_image_labels.numpy())))))\n",
    "print(train_image_labels.shape, \" \", train_image_embeddings_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f65f21-3982-4557-b1ff-3dd06b45ddfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def find_indices(list_to_check, item_to_find):\n",
    "    return [idx for idx, value in enumerate(list_to_check) if value == item_to_find]\n",
    "  \n",
    "image_label_list = list(train_image_labels.numpy())\n",
    "classes_to_index = {i: [] for i in range(200)}\n",
    "for key in classes_to_index.keys():\n",
    "    indexes = find_indices(image_label_list, key)\n",
    "    random_40 = random.sample(indexes,40)\n",
    "    classes_to_index[key] = random_40\n",
    "\n",
    "print(classes_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93846d1-7e91-4acb-8682-8a302b87bd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = np.zeros((200,200,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d90870-bd49-4161-a01f-e6055bd2c8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim = nn.CosineSimilarity(dim=0, eps=1e-8)\n",
    "print(train_image_embeddings[0].shape)\n",
    "cosine_sim(train_image_embeddings[2957], train_image_embeddings[20364].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cc3d3a-517e-4484-a7af-0069bd7ff46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "final_indices = list(classes_to_index.values())\n",
    "final_indices = sorted([item for sublist in final_indices for item in sublist])\n",
    "print(final_indices)\n",
    "print(len(final_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583322c5-2f37-4f22-80fd-60d044ac9643",
   "metadata": {},
   "outputs": [],
   "source": [
    "for act_i in tqdm(range(len(final_indices))):\n",
    "    for act_j in range(act_i+1, len(final_indices)):\n",
    "        i, j = final_indices[act_i], final_indices[act_j]\n",
    "        class_i, class_j = train_image_labels[i], train_image_labels[j]\n",
    "        emb_i, emb_j = train_image_embeddings[i], train_image_embeddings[j]\n",
    "        sim_score = cosine_sim(emb_i, emb_j).detach().item()\n",
    "    \n",
    "        if class_i != class_j:\n",
    "            similarity_matrix[class_i][class_j][0] += sim_score\n",
    "            similarity_matrix[class_i][class_j][1] += 1\n",
    "\n",
    "            similarity_matrix[class_j][class_i][0] += sim_score\n",
    "            similarity_matrix[class_j][class_i][1] += 1\n",
    "        else:\n",
    "            similarity_matrix[class_i][class_i][0] += sim_score\n",
    "            similarity_matrix[class_i][class_i][1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdd8e99-9361-4b32-a1b8-587fbf9a55c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sim = np.zeros((200,200))\n",
    "for i in range(200):\n",
    "    for j in range(200):\n",
    "        if i == j:\n",
    "            avg_sim[i][j] = similarity_matrix[i][j][0]/similarity_matrix[i][j][1] + 0.0225\n",
    "        else:\n",
    "            avg_sim[i][j] = similarity_matrix[i][j][0]/similarity_matrix[i][j][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be271b9-f479-41b1-bd8d-17c895032140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "x_distribution = F.softmax(torch.tensor(avg_sim), dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b157ad-401e-4c15-a8b2-fd84fcaa10c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(len(x_distribution)):\n",
    "    if i == torch.argmax(x_distribution[i]):\n",
    "        count += 1\n",
    "        # print(i)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e066c97-7b2c-4b76-adec-461bb5d9de7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(avg_sim, './tiny_imagenet_ae_simmatrix.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
