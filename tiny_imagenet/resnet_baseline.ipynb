{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241de925-fd28-4329-b206-909cd0b23963",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0729ef-0db7-4ffb-8239-07781e35da0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -qq 'tiny-imagenet-200.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e28508-8ce4-4ef5-836c-cd8845c7c020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221c1c15-2336-4f0b-93f7-c426236c6b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset, random_split\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import transforms as T\n",
    "from torchvision import models, datasets\n",
    "from random import randint\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382fda49-1ed2-4dcb-87f3-2cfa1e638619",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea480a0e-b88f-47a7-a71e-cf44883ebdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'tiny-imagenet-200' # Original images come in shapes of [3,64,64]\n",
    "\n",
    "# Define training and validation data paths\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train') \n",
    "TEST_DIR = os.path.join(DATA_DIR, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7e8361-3188-4cd4-af64-ebc574d13b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "def show_batch(dataloader):\n",
    "    dataiter = iter(dataloader)\n",
    "    images, labels = dataiter.next()    \n",
    "    imshow(make_grid(images)) # Using Torchvision.utils make_grid function\n",
    "    \n",
    "def show_image(dataloader):\n",
    "    dataiter = iter(dataloader)\n",
    "    images, labels = dataiter.next()\n",
    "    random_num = randint(0, len(images)-1)\n",
    "    imshow(images[random_num])\n",
    "    label = labels[random_num]\n",
    "    print(f'Label: {label}, Shape: {images[random_num].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a1b31f-b31b-479f-abc3-c3546e9f13c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup function to create dataloaders for image datasets\n",
    "def generate_dataloader(data, name, transform, batch_size=32, val_split=None):\n",
    "    if data is None: \n",
    "        return None\n",
    "    \n",
    "    # Read image files to pytorch dataset using ImageFolder, a generic data \n",
    "    # loader where images are in format root/label/filename\n",
    "    # See https://pytorch.org/vision/stable/datasets.html\n",
    "    if transform is None:\n",
    "        dataset = datasets.ImageFolder(data, transform=T.ToTensor())\n",
    "    else:\n",
    "        dataset = datasets.ImageFolder(data, transform=transform)\n",
    "\n",
    "    if val_split:\n",
    "        val_ds_length = int(len(dataset)*val_split)\n",
    "        train_ds_length = len(dataset) - val_ds_length\n",
    "        train_split_dataset, val_split_dataset = random_split(dataset, [train_ds_length, val_ds_length], generator=torch.Generator().manual_seed(42))\n",
    "        print(f'Number of training samples: {len(train_split_dataset)}, Number of unique train labels: {len(set(train_split_dataset.dataset.targets))}')\n",
    "        print(f'Number of validation samples: {len(val_split_dataset)}, Number of unique validation labels: {len(set(val_split_dataset.dataset.targets))}')\n",
    "\n",
    "    # Set options for device\n",
    "    if use_cuda:\n",
    "        kwargs = {\"pin_memory\": True, \"num_workers\": 1}\n",
    "    else:\n",
    "        kwargs = {}\n",
    "    \n",
    "    if not val_split:\n",
    "      # Wrap image dataset (defined above) in dataloader \n",
    "      dataloader = DataLoader(dataset, batch_size=batch_size, \n",
    "                          shuffle=(name==\"train\"), \n",
    "                          **kwargs)\n",
    "      \n",
    "      return dataloader\n",
    "    else:\n",
    "        train_dataloader = DataLoader(train_split_dataset, batch_size=batch_size, \n",
    "                          shuffle=(name==\"train\"), \n",
    "                          **kwargs)\n",
    "        val_dataloader = DataLoader(val_split_dataset, batch_size=batch_size, \n",
    "                          shuffle=(name==\"train\"), \n",
    "                          **kwargs)\n",
    "        return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7322a0be-7742-4899-9897-a307a9408cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate validation subfolders for the validation images based on\n",
    "# their labels indicated in the val_annotations txt file\n",
    "test_img_dir = os.path.join(TEST_DIR, 'images')\n",
    "\n",
    "# Open and read val annotations text file\n",
    "fp = open(os.path.join(TEST_DIR, 'val_annotations.txt'), 'r')\n",
    "data = fp.readlines()\n",
    "\n",
    "# Create dictionary to store img filename (word 0) and corresponding\n",
    "# label (word 1) for every line in the txt file (as key value pair)\n",
    "val_img_dict = {}\n",
    "for line in data:\n",
    "    words = line.split('\\t')\n",
    "    val_img_dict[words[0]] = words[1]\n",
    "fp.close()\n",
    "\n",
    "# Display first 10 entries of resulting val_img_dict dictionary\n",
    "print({k: val_img_dict[k] for k in list(val_img_dict)[:10]})\n",
    "\n",
    "# Create subfolders (if not present) for validation images based on label,\n",
    "# and move images into the respective folders\n",
    "for img, folder in val_img_dict.items():\n",
    "    newpath = (os.path.join(test_img_dir, folder))\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    if os.path.exists(os.path.join(test_img_dir, img)):\n",
    "        os.rename(os.path.join(test_img_dir, img), os.path.join(newpath, img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e45ea35-bb25-4d50-81e5-874281e28aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp_data = datasets.ImageFolder('tiny-imagenet-200/train')\n",
    "\n",
    "# Stick all the images together to form a 1600000 X 32 X 3 array\n",
    "x = np.concatenate([np.asarray(temp_data[i][0]) for i in range(len(temp_data))])\n",
    "\n",
    "# calculate the mean and std along the (0, 1) axes\n",
    "mean = np.mean(x, axis=(0, 1))/255\n",
    "std = np.std(x, axis=(0, 1))/255\n",
    "# the the mean and std\n",
    "mean=mean.tolist()\n",
    "std=std.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87b9dee-ce53-497f-a641-e113cd2bcd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation - optional depending on future resnet implementation\n",
    "\n",
    "# Define transformation sequence for image pre-processing\n",
    "# If not using pre-trained model, normalize with 0.5, 0.5, 0.5 (mean and SD)\n",
    "# If using pre-trained ImageNet, normalize with mean=[0.485, 0.456, 0.406], \n",
    "# std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    # T.Resize(256), # Resize images to 256 x 256\n",
    "    # T.CenterCrop(224), # Center crop image\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomRotation(15),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "test_transform = T.Compose([\n",
    "                # T.Resize(256), # Resize images to 256 x 256\n",
    "                # T.CenterCrop(224), # Center crop image\n",
    "                # T.RandomHorizontalFlip(),\n",
    "                T.ToTensor(),  # Converting cropped images to tensors\n",
    "                T.Normalize(mean, std)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2c834a-fa65-40bf-875e-74743227c980",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ee3a2a-af26-4f43-bb30-5c7e67b10bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL.Image import NONE\n",
    "#\n",
    "# Define batch size for DataLoaders\n",
    "batch_size = 128\n",
    "\n",
    "# Create DataLoaders for pre-trained models (normalized based on specific requirements)\n",
    "train_loader = generate_dataloader(TRAIN_DIR, \"train\",\n",
    "                                  transform=train_transform, batch_size=batch_size)\n",
    "\n",
    "test_loader = generate_dataloader(test_img_dir, \"val\",\n",
    "                                 transform=test_transform, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b1f59e-8df4-4610-aac1-9c5f6e07de6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083f7c33-d960-4a62-98ff-8101d914929b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(2048*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18(num_classes=10):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
    "\n",
    "def ResNet50(num_classes=10):\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes)\n",
    "\n",
    "\n",
    "def test():\n",
    "    net = ResNet18()\n",
    "    y = net(torch.randn(1, 3, 64, 64))\n",
    "    print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3bff48-7673-44a9-9d0f-7ff4112d0c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ad47c2-9328-4596-a872-c496b977f17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    'train': train_loader,\n",
    "    'val': test_loader,\n",
    "    'test': test_loader\n",
    "}\n",
    "dataset_sizes = {\n",
    "    'train': len(train_loader.dataset),\n",
    "    'val': len(test_loader.dataset),\n",
    "    'test': len(test_loader.dataset),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73121a5d-3ea2-4bea-b751-984ad84f3bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in tqdm(dataloaders[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                print('Saving..')\n",
    "                state = {\n",
    "                    'model': model.state_dict(),\n",
    "                    'acc': epoch_acc,\n",
    "                    'epoch': epoch,\n",
    "                }\n",
    "                # if not os.path.isdir('checkpoint'):\n",
    "                #     os.mkdir('checkpoint')\n",
    "                torch.save(state, './resnet_18_tinyimagenet.pth')\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63b2180-1de9-439e-b7ab-c86a2af4b500",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "model_ft = ResNet18(num_classes=200)\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "exp_lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer_ft, milestones=[60, 120, 160], gamma=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bfbd56-11f8-4985-ac41-66ba307c5acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61568cec-a100-4210-9a58-92150dac9b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b112e39-bf85-4857-8fc7-83e128ae3e17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
