{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab086d1-b67f-4c0e-8f6e-e6bdac8ef111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954fb8f6-0dc9-4059-a6e7-ca3814b984c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset, random_split\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import transforms as T\n",
    "from torchvision import models, datasets\n",
    "from random import randint\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa296335-cd1d-4b78-9d4c-e8f94a036972",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e67d22e-39a7-4a08-828f-14e3db9fd41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'tiny-imagenet-200' # Original images come in shapes of [3,64,64]\n",
    "\n",
    "# Define training and validation data paths\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train') \n",
    "TEST_DIR = os.path.join(DATA_DIR, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dd4bc1-017a-4309-836f-10ee39e72667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "def show_batch(dataloader):\n",
    "    dataiter = iter(dataloader)\n",
    "    images, labels = dataiter.next()    \n",
    "    imshow(make_grid(images)) # Using Torchvision.utils make_grid function\n",
    "    \n",
    "def show_image(dataloader):\n",
    "    dataiter = iter(dataloader)\n",
    "    images, labels = dataiter.next()\n",
    "    random_num = randint(0, len(images)-1)\n",
    "    imshow(images[random_num])\n",
    "    label = labels[random_num]\n",
    "    print(f'Label: {label}, Shape: {images[random_num].shape}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd29023-213f-4541-b128-a9219719b3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup function to create dataloaders for image datasets\n",
    "def generate_dataloader(data, name, transform, batch_size=32, val_split=None):\n",
    "    if data is None: \n",
    "        return None\n",
    "    \n",
    "    # Read image files to pytorch dataset using ImageFolder, a generic data \n",
    "    # loader where images are in format root/label/filename\n",
    "    # See https://pytorch.org/vision/stable/datasets.html\n",
    "    if transform is None:\n",
    "        dataset = datasets.ImageFolder(data, transform=T.ToTensor())\n",
    "    else:\n",
    "        dataset = datasets.ImageFolder(data, transform=transform)\n",
    "\n",
    "    if val_split:\n",
    "        val_ds_length = int(len(dataset)*val_split)\n",
    "        train_ds_length = len(dataset) - val_ds_length\n",
    "        train_split_dataset, val_split_dataset = random_split(dataset, [train_ds_length, val_ds_length], generator=torch.Generator().manual_seed(42))\n",
    "        print(f'Number of training samples: {len(train_split_dataset)}, Number of unique train labels: {len(set(train_split_dataset.dataset.targets))}')\n",
    "        print(f'Number of validation samples: {len(val_split_dataset)}, Number of unique validation labels: {len(set(val_split_dataset.dataset.targets))}')\n",
    "\n",
    "    # Set options for device\n",
    "    if use_cuda:\n",
    "        kwargs = {\"pin_memory\": True, \"num_workers\": 1}\n",
    "    else:\n",
    "        kwargs = {}\n",
    "    \n",
    "    if not val_split:\n",
    "      # Wrap image dataset (defined above) in dataloader \n",
    "      dataloader = DataLoader(dataset, batch_size=batch_size, \n",
    "                          shuffle=(name==\"train\"), \n",
    "                          **kwargs)\n",
    "      \n",
    "      return dataloader\n",
    "    else:\n",
    "        train_dataloader = DataLoader(train_split_dataset, batch_size=batch_size, \n",
    "                          shuffle=(name==\"train\"), \n",
    "                          **kwargs)\n",
    "        val_dataloader = DataLoader(val_split_dataset, batch_size=batch_size, \n",
    "                          shuffle=(name==\"train\"), \n",
    "                          **kwargs)\n",
    "        return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3782053-5f7f-4ee2-98fa-c4f79bfb3b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate validation subfolders for the validation images based on\n",
    "# their labels indicated in the val_annotations txt file\n",
    "test_img_dir = os.path.join(TEST_DIR, 'images')\n",
    "\n",
    "# Open and read val annotations text file\n",
    "fp = open(os.path.join(TEST_DIR, 'val_annotations.txt'), 'r')\n",
    "data = fp.readlines()\n",
    "\n",
    "# Create dictionary to store img filename (word 0) and corresponding\n",
    "# label (word 1) for every line in the txt file (as key value pair)\n",
    "val_img_dict = {}\n",
    "for line in data:\n",
    "    words = line.split('\\t')\n",
    "    val_img_dict[words[0]] = words[1]\n",
    "fp.close()\n",
    "\n",
    "# Display first 10 entries of resulting val_img_dict dictionary\n",
    "print({k: val_img_dict[k] for k in list(val_img_dict)[:10]})\n",
    "\n",
    "# Create subfolders (if not present) for validation images based on label,\n",
    "# and move images into the respective folders\n",
    "for img, folder in val_img_dict.items():\n",
    "    newpath = (os.path.join(test_img_dir, folder))\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    if os.path.exists(os.path.join(test_img_dir, img)):\n",
    "        os.rename(os.path.join(test_img_dir, img), os.path.join(newpath, img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0e70a0-1aef-49a6-a650-1b04578c77c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.4802369434455423, 0.4480670369274663, 0.39750364210899203]\n",
    "std = [0.2764364260576532, 0.268863276261085, 0.28158992530131444]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3e2400-2353-4c1c-9720-29260ed8e9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation - optional depending on future resnet implementation\n",
    "\n",
    "# Define transformation sequence for image pre-processing\n",
    "# If not using pre-trained model, normalize with 0.5, 0.5, 0.5 (mean and SD)\n",
    "# If using pre-trained ImageNet, normalize with mean=[0.485, 0.456, 0.406], \n",
    "# std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    # T.Resize(32), # Resize images to 256 x 256\n",
    "    # T.CenterCrop(224), # Center crop image\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomRotation(15),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "test_transform = T.Compose([\n",
    "                # T.Resize(32), # Resize images to 256 x 256\n",
    "                # T.CenterCrop(224), # Center crop image\n",
    "                # T.RandomHorizontalFlip(),\n",
    "                T.ToTensor(),  # Converting cropped images to tensors\n",
    "                T.Normalize(mean, std)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bb08c5-27d3-4ae5-bb5b-723251b69dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389d5096-c6f8-4a6b-a73a-ff0a87d3614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL.Image import NONE\n",
    "#\n",
    "# Define batch size for DataLoaders\n",
    "batch_size = 128\n",
    "\n",
    "# Create DataLoaders for pre-trained models (normalized based on specific requirements)\n",
    "train_loader = generate_dataloader(TRAIN_DIR, \"train\",\n",
    "                                  transform=train_transform, batch_size=batch_size)\n",
    "\n",
    "test_loader = generate_dataloader(test_img_dir, \"val\",\n",
    "                                 transform=test_transform, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9909ea68-ff56-4f89-bb32-f59e5ae7c556",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e096e5ba-fd73-4bd1-80aa-6be2964b625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(2048*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18(num_classes=10):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
    "\n",
    "def ResNet50(num_classes=10):\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes)\n",
    "\n",
    "\n",
    "def test():\n",
    "    net = ResNet18()\n",
    "    y = net(torch.randn(1, 3, 64, 64))\n",
    "    print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c959f5-5b0b-494e-9dd7-69f6d47605bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7830e9c-758e-4bfa-a309-4542417dc619",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    'train': train_loader,\n",
    "    'val': test_loader,\n",
    "    'test': test_loader\n",
    "}\n",
    "dataset_sizes = {\n",
    "    'train': len(train_loader.dataset),\n",
    "    'val': len(test_loader.dataset),\n",
    "    'test': len(test_loader.dataset),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f268e76-4c4c-4aec-a093-6b9298a6cd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_kd(outputs, labels, teacher_outputs, **kwargs):\n",
    "    \"\"\"\n",
    "    Compute the knowledge-distillation (KD) loss given outputs, labels.\n",
    "    \"Hyperparameters\": temperature and alpha\n",
    "    NOTE: the KL Divergence for PyTorch comparing the softmaxs of teacher\n",
    "    and student expects the input tensor to be log probabilities! See Issue #2\n",
    "    \"\"\"\n",
    "    teacher_outputs = teacher_outputs.double()\n",
    "    alpha = kwargs['alpha']\n",
    "    T = kwargs['temperature']\n",
    "\n",
    "    loss_CE = F.cross_entropy(outputs, labels)\n",
    "    D_KL = nn.KLDivLoss()(F.log_softmax(outputs/T, dim=1), teacher_outputs) * (T * T)\n",
    "    KD_loss =  (1. - alpha)*loss_CE + alpha*D_KL\n",
    "\n",
    "    return KD_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6b75f1-e72b-4ada-932b-f62d2f4b1cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_probabilities = torch.tensor(torch.load('./tiny_imagenet_ae_simmatrix.pt'))\n",
    "soft_probabilities = soft_probabilities.to(device)\n",
    "soft_probabilities = F.softmax(torch.tensor(soft_probabilities), dim = 1)\n",
    "# print(soft_probabilities.shape)\n",
    "# print(soft_probabilities.type())\n",
    "\n",
    "print(soft_probabilities[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61fae5a-faef-40c0-a4f5-69a498652ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "\n",
    "def train_model(model, criterion_kd, optimizer, scheduler, alpha, num_epochs=25, temp=1):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in tqdm(dataloaders[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                teacher_outputs = soft_probabilities[labels]\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    outputs = outputs.double()\n",
    "                    # print(outputs.type())\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion_kd(outputs, labels, teacher_outputs, alpha=alpha, temperature=temp)\n",
    "                    # print(loss.type())\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                print('Saving..')\n",
    "                state = {\n",
    "                    'model': model.state_dict(),\n",
    "                    'acc': epoch_acc,\n",
    "                    'epoch': epoch,\n",
    "                }\n",
    "                # if not os.path.isdir('checkpoint'):\n",
    "                #     os.mkdir('checkpoint')\n",
    "                torch.save(state, './tiny_imagenet_resnet18_ae_distilled.pth')\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db92e45f-37c1-45fe-a295-93c8c46a4739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "\n",
    "def accuracyAtk(output, target, topk=(1,5)):\n",
    "    \"\"\"\n",
    "    Computes the accuracy over the k top predictions for the specified values of k\n",
    "    In top-5 accuracy you give yourself credit for having the right answer\n",
    "    if the right answer appears in your top five guesses.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = (pred == target.unsqueeze(dim=0)).expand_as(pred)\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k)\n",
    "        return res\n",
    "\n",
    "def eval_model(model):\n",
    "    since = time.time()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    final_top_1, final_top_5 = 0,0\n",
    "\n",
    "    for inputs, labels in tqdm(dataloaders['test']):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            top1, top5 = accuracyAtk(outputs, labels)\n",
    "            final_top_1 += top1\n",
    "            final_top_5 += top5\n",
    "    \n",
    "    top1_acc = final_top_1/dataset_sizes['test']\n",
    "    top5_acc = final_top_5/dataset_sizes['test']\n",
    "    print(f'Top 1 test accuracy = {top1_acc}, Top 5 test accuract = {top5_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21f19dc-21bd-4e9a-b25a-a5db06d5fc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "for t in [20, 10, 5, 1]:\n",
    "    for i in range(1, 10, 1):\n",
    "        if t == 20 and i in [6, 7, 8]:\n",
    "            continue\n",
    "        model_ft = ResNet18(num_classes=200)\n",
    "        model_ft = model_ft.to(device)\n",
    "\n",
    "        criterion = loss_fn_kd\n",
    "        optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "        exp_lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer_ft, milestones=[60, 120, 160], gamma=0.2)\n",
    "        model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, i/10, num_epochs=200, temp=t)\n",
    "        eval_model(model_ft)\n",
    "        del model_ft\n",
    "    print(\"-------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d586d6-3296-4d00-baf5-de97f246b3df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
