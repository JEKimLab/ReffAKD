{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9939254-6928-4aee-96df-ee310e8a1886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb1eb03-0f91-47f5-8822-8d02b85bab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset, random_split\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import transforms as T\n",
    "from torchvision import models, datasets\n",
    "from random import randint\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980dd5e0-cbdb-4b51-9f6b-6091a2ad1660",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faca389-8e68-431c-aecd-0e9a874fd3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = datasets.FashionMNIST('./', train=True, download=True)\n",
    "\n",
    "# Stick all the images together to form a 600000 X 28 array\n",
    "x = np.concatenate([np.asarray(train_data[i][0]) for i in range(len(train_data))])\n",
    "\n",
    "# calculate the mean and std along the (0, 1) axes\n",
    "mean = np.mean(x, axis=(0, 1))/255\n",
    "std = np.std(x, axis=(0, 1))/255\n",
    "# the the mean and std\n",
    "mean=mean.tolist()\n",
    "std=std.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bad772d-9dec-4b42-a2d3-bf7270522b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'mean = {mean}, std = {std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e95125-840a-445a-aa27-0e8844e02b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "def show_batch(dataloader):\n",
    "    dataiter = iter(dataloader)\n",
    "    images, labels = next(dataiter)    \n",
    "    imshow(make_grid(images)) # Using Torchvision.utils make_grid function\n",
    "    \n",
    "def show_image(dataloader):\n",
    "    dataiter = iter(dataloader)\n",
    "    images, labels = next(dataiter)\n",
    "    random_num = randint(0, len(images)-1)\n",
    "    imshow(images[random_num])\n",
    "    label = labels[random_num]\n",
    "    print(f'Label: {label}, Shape: {images[random_num].shape}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5577a8c3-61a4-4c18-9546-0602bc48067c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation - optional depending on future resnet implementation\n",
    "\n",
    "# Define transformation sequence for image pre-processing\n",
    "# If not using pre-trained model, normalize with 0.5, 0.5, 0.5 (mean and SD)\n",
    "# If using pre-trained ImageNet, normalize with mean=[0.485, 0.456, 0.406], \n",
    "# std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    T.Resize((32,32)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomRotation(15),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "test_transform = T.Compose([\n",
    "                # T.Resize(256), # Resize images to 256 x 256\n",
    "                # T.CenterCrop(224), # Center crop image\n",
    "                # T.RandomHorizontalFlip(),\n",
    "                T.Resize((32,32)),\n",
    "                T.ToTensor(),  # Converting cropped images to tensors\n",
    "                T.Normalize(mean, std)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c044e73-50f2-4656-b930-6c6343f387e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066da8bc-f0bb-4a5a-a571-6814ff61ecab",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = datasets.FashionMNIST(\"./\",\n",
    "                                         train=True,\n",
    "                                         download=True,\n",
    "                                         transform=train_transform)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size, shuffle=True, num_workers=2,pin_memory=True)\n",
    "\n",
    "testset = datasets.FashionMNIST(\"./\",\n",
    "                                        train=False,\n",
    "                                        download=True,\n",
    "                                        transform=test_transform)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size*2,pin_memory=True, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd23b5d-c3cd-4989-bc33-76c5a764a234",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8558b0-ab48-4161-a9bf-5a69360f59ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "    \n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Input size: [batch, 3, 32, 32]\n",
    "        # Output size: [batch, 3, 32, 32]\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 12, 4, stride=2, padding=1),            # [batch, 12, 16, 16]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(12, 24, 4, stride=2, padding=1),           # [batch, 24, 8, 8]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(24, 48, 4, stride=2, padding=1),           # [batch, 48, 4, 4]\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(48, 24, 4, stride=2, padding=1),  # [batch, 24, 8, 8]\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(24, 12, 4, stride=2, padding=1),  # [batch, 12, 16, 16]\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(12, 1, 4, stride=2, padding=1),   # [batch, 3, 32, 32]\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded, encoded.view(-1, 48*4*4)\n",
    "\n",
    "\n",
    "def test():\n",
    "    net = Autoencoder()\n",
    "    net = net.to(device)\n",
    "    item = iter(train_loader)\n",
    "    img, _ = next(item)\n",
    "\n",
    "    img = img[0]\n",
    "    img = img[None, :]\n",
    "    img = img.to(device)\n",
    "    img_cons, rep  = net(img)\n",
    "    std_t = torch.tensor(std).to(device)\n",
    "    mean_t = torch.tensor(mean).to(device)\n",
    "    img_cons = img_cons * std_t + mean_t\n",
    "    img = img * std_t + mean_t\n",
    "    # print(rep)\n",
    "    print(rep.shape, \"  \", img_cons.shape)\n",
    "    temp_r = imshow(img.cpu()[0])\n",
    "    imshow(img_cons.detach().cpu()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1650b48-7bbe-457b-af07-7c02e7ff5226",
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12444564-0b8f-46dd-9fc8-97df8dcdf543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(Autoencoder(512), input_size=(1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64e1bb4-7984-487a-adef-b5ccc6930711",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    'train': train_loader,\n",
    "    'val': test_loader,\n",
    "    'test': test_loader\n",
    "}\n",
    "dataset_sizes = {\n",
    "    'train': len(train_loader.dataset),\n",
    "    'val': len(test_loader.dataset),\n",
    "    'test': len(test_loader.dataset),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655aa4c4-3e4f-432d-87ae-d1347fbbb8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "from torch import autograd\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train','val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, _ in tqdm(dataloaders[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    reconstructed_img, hidden_rep = model(inputs)\n",
    "                    \n",
    "                    # l1_loss = sparse_loss(model, inputs) * 0.001\n",
    "                    loss = criterion(reconstructed_img, inputs)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        with autograd.detect_anomaly():\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "#                         for name, param in model_ft.named_parameters():\n",
    "#                             print(name, param.grad.norm())\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss}')          \n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "#             if phase == 'val':\n",
    "                print('Saving..')\n",
    "                state = {\n",
    "                    'model': model.state_dict(),\n",
    "                    'loss': epoch_loss,\n",
    "                    'epoch': epoch,\n",
    "                }\n",
    "                # if not os.path.isdir('checkpoint'):\n",
    "                #     os.mkdir('checkpoint')\n",
    "                torch.save(state, './fashion_mnist_ae_tiny.pth')\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val loss: {best_loss:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c728be5-d4f7-4fbc-acdd-d67093b0c740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "model_ft = Autoencoder()\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "exp_lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer_ft, milestones=[60, 120, 160], gamma=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a9ce4e-7f69-4270-9cc3-e281cb288619",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22338690-49d4-42ab-89a4-76cd23ce30a5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "device = 'cuda'\n",
    "model_ft.eval()\n",
    "item = iter(train_loader)\n",
    "img, _ = next(item)\n",
    "\n",
    "img = img[0]\n",
    "img = img[None, :]\n",
    "img = img.to(device)\n",
    "img_cons, rep  = model_ft(img)\n",
    "std_t = torch.tensor(std).to(device)\n",
    "mean_t = torch.tensor(mean).to(device)\n",
    "img_cons = img_cons * std_t + mean_t\n",
    "img = img * std_t + mean_t\n",
    "print(rep.shape, \"  \", img_cons.shape)\n",
    "temp_r = imshow(img.cpu()[0])\n",
    "imshow(img_cons.detach().cpu()[0])\n",
    "print(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed449704-7a62-4311-88aa-9b329c5d8a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_embeddings = []\n",
    "train_image_labels = []\n",
    "\n",
    "def populate_embedding_lable_list(model):\n",
    "    global train_image_embeddings, train_image_labels\n",
    "\n",
    "    model.eval()   # Set model to evaluate mode\n",
    "    # Iterate over data.\n",
    "    for inputs, labels in tqdm(dataloaders['train']):\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        # forward\n",
    "        # track history if only in train\n",
    "        with torch.set_grad_enabled(False):\n",
    "            reconstructed_img, hidden_rep = model(inputs)\n",
    "            train_image_embeddings.extend(hidden_rep)\n",
    "            train_image_labels.extend(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b63927-a279-4d88-a842-808dcd2acf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "populate_embedding_lable_list(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4a74d8-cb27-4ecf-9c10-45aa32b2184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_labels = torch.flatten(torch.tensor(train_image_labels))\n",
    "# torch.save(train_image_labels, '/content/drive/MyDrive/TDML_Project/train_image_labels.pt')\n",
    "\n",
    "train_image_embeddings_tensor = torch.stack(train_image_embeddings)\n",
    "# torch.save(train_image_embeddings_tensor, '/content/drive/MyDrive/TDML_Project/train_image_embeddings_tensor.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d96bce-43c4-43ef-9bb7-e8a4b812693e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sorted(set(list(train_loader.dataset.targets)))))\n",
    "print(len(sorted(set(list(train_image_labels.numpy())))))\n",
    "print(train_image_labels.shape, \" \", train_image_embeddings_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4f5280-7026-4eee-9791-bbbe70d5036e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def find_indices(list_to_check, item_to_find):\n",
    "    return [idx for idx, value in enumerate(list_to_check) if value == item_to_find]\n",
    "  \n",
    "image_label_list = list(train_image_labels.numpy())\n",
    "classes_to_index = {i: [] for i in range(10)}\n",
    "for key in classes_to_index.keys():\n",
    "    indexes = find_indices(image_label_list, key)\n",
    "    random_40 = random.sample(indexes,40)\n",
    "    classes_to_index[key] = random_40\n",
    "\n",
    "print(classes_to_index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feda8ab-729d-40cf-907f-9213549b22e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "final_indices = list(classes_to_index.values())\n",
    "final_indices = sorted([item for sublist in final_indices for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eac6ef4-a2f5-4cca-8910-d945212f08e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = np.zeros((10,10,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3be7d4c-8a49-45e7-815b-a43c9254eec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim = nn.CosineSimilarity(dim=0, eps=1e-8)\n",
    "print(train_image_embeddings[0].shape)\n",
    "cosine_sim(train_image_embeddings[2957], train_image_embeddings[20364].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b66485f-d5c0-4b1d-a39d-761fd5e4f970",
   "metadata": {},
   "outputs": [],
   "source": [
    "for act_i in tqdm(range(len(final_indices))):\n",
    "    for act_j in range(act_i+1, len(final_indices)):\n",
    "        i, j = final_indices[act_i], final_indices[act_j]\n",
    "        class_i, class_j = train_image_labels[i], train_image_labels[j]\n",
    "        emb_i, emb_j = train_image_embeddings[i], train_image_embeddings[j]\n",
    "        sim_score = cosine_sim(emb_i, emb_j).detach().item()\n",
    "    \n",
    "        if class_i != class_j:\n",
    "            similarity_matrix[class_i][class_j][0] += sim_score\n",
    "            similarity_matrix[class_i][class_j][1] += 1\n",
    "\n",
    "            similarity_matrix[class_j][class_i][0] += sim_score\n",
    "            similarity_matrix[class_j][class_i][1] += 1\n",
    "        else:\n",
    "            similarity_matrix[class_i][class_i][0] += sim_score\n",
    "            similarity_matrix[class_i][class_i][1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcaef00-0c61-4ec6-a852-82004e9ed9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sim = np.zeros((10,10))\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        if i == j:\n",
    "            avg_sim[i][j] = similarity_matrix[i][j][0]/similarity_matrix[i][j][1] + 0.008\n",
    "        else:\n",
    "            avg_sim[i][j] = similarity_matrix[i][j][0]/similarity_matrix[i][j][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cbae11-1359-4fd9-acdc-cae8fbb2f05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "x_distribution = F.softmax(torch.tensor(avg_sim), dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12bdb11-ce73-4704-9b25-b6c375776cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_distribution[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b227f26d-a743-4e9c-b698-c16a1f49040d",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(len(x_distribution)):\n",
    "    if i == torch.argmax(x_distribution[i]):\n",
    "        count += 1\n",
    "        # print(i)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87ea7a7-a9a2-4100-9740-3c33a84e6bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(avg_sim, './tiny_ae_fashion_mnist.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbb47f5-7d94-4895-bf2d-16b06d8f5478",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
