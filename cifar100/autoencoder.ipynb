{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Col5bN65h4M3"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XlTzPfoK4IHf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset, random_split\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import transforms as T\n",
    "from torchvision import models, datasets\n",
    "from random import randint\n",
    "from torchinfo import summary\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iYmUacr6a3-y",
    "outputId": "2f3910b4-2a81-4407-f593-ee541076d42b"
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85,
     "referenced_widgets": [
      "9957632d96884fd68bd949a3c374bc26",
      "918329870f324a37b7c886307130e4f0",
      "a58b59067c224f68b2224372f49c8c37",
      "c0284820254443aa882594ff94508904",
      "e338f0bd470a42f48478d564c8438c34",
      "3f75f21085d34e3a8f8400fdcc663643",
      "4688d70dd6be4a3e90b45367149f5aac",
      "32e893a1b4f24c2e9b7ace0db3715aec",
      "d6118f6d47074f6385ef881220c3d0d8",
      "a176b2769214468990fc93ec6f64d83c",
      "f8cb745f778a45ae8711f52ca3ca89ec"
     ]
    },
    "id": "cVLrKrzYa1sy",
    "outputId": "75f78c0c-38ec-4d56-a79c-ec2042f3eda3"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_data = datasets.CIFAR100('./', train=True, download=True)\n",
    "\n",
    "# Stick all the images together to form a 1600000 X 32 X 3 array\n",
    "x = np.concatenate([np.asarray(train_data[i][0]) for i in range(len(train_data))])\n",
    "\n",
    "# calculate the mean and std along the (0, 1) axes\n",
    "mean = np.mean(x, axis=(0, 1))/255\n",
    "std = np.std(x, axis=(0, 1))/255\n",
    "# the the mean and std\n",
    "mean=mean.tolist()\n",
    "std=std.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YF7hVP9Y5SoY"
   },
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "def show_batch(dataloader):\n",
    "    dataiter = iter(dataloader)\n",
    "    images, labels = next(dataiter)    \n",
    "    imshow(make_grid(images)) # Using Torchvision.utils make_grid function\n",
    "    \n",
    "def show_image(dataloader):\n",
    "    dataiter = iter(dataloader)\n",
    "    images, labels = next(dataiter)\n",
    "    random_num = randint(0, len(images)-1)\n",
    "    imshow(images[random_num])\n",
    "    print(images[random_num])\n",
    "    label = labels[random_num]\n",
    "    print(f'Label: {label}, Shape: {images[random_num].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0XODO7Wf8UWU"
   },
   "outputs": [],
   "source": [
    "# Transformation - optional depending on future resnet implementation\n",
    "\n",
    "# Define transformation sequence for image pre-processing\n",
    "# If not using pre-trained model, normalize with 0.5, 0.5, 0.5 (mean and SD)\n",
    "# If using pre-trained ImageNet, normalize with mean=[0.485, 0.456, 0.406], \n",
    "# std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomRotation(15),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std, inplace=True)\n",
    "])\n",
    "\n",
    "test_transform = T.Compose([\n",
    "                # T.Resize(256), # Resize images to 256 x 256\n",
    "                # T.CenterCrop(224), # Center crop image\n",
    "                # T.RandomHorizontalFlip(),\n",
    "                T.ToTensor(),  # Converting cropped images to tensors\n",
    "                T.Normalize(mean, std)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xlrFdSdwbB8b"
   },
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KavJHz-I76jz",
    "outputId": "0087d9c9-53f8-4237-a6b9-0941371e07c7"
   },
   "outputs": [],
   "source": [
    "trainset = datasets.CIFAR100(\"./\",\n",
    "                                         train=True,\n",
    "                                         download=True,\n",
    "                                         transform=train_transform)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size, shuffle=True, num_workers=2,pin_memory=True)\n",
    "\n",
    "testset = datasets.CIFAR100(\"./\",\n",
    "                                        train=False,\n",
    "                                        download=True,\n",
    "                                        transform=test_transform)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size*2,pin_memory=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "id": "tb7ZxHtv8kvm",
    "outputId": "ab8977a3-fcea-491a-ffc0-7241854df187"
   },
   "outputs": [],
   "source": [
    "show_image(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9r1nYp4J87uo"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "    \n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Input size: [batch, 3, 32, 32]\n",
    "        # Output size: [batch, 3, 32, 32]\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 12, 4, stride=2, padding=1),            # [batch, 12, 16, 16]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(12, 24, 4, stride=2, padding=1),           # [batch, 24, 8, 8]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(24, 48, 4, stride=2, padding=1),           # [batch, 48, 4, 4]\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(48, 24, 4, stride=2, padding=1),  # [batch, 24, 8, 8]\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(24, 12, 4, stride=2, padding=1),  # [batch, 12, 16, 16]\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(12, 3, 4, stride=2, padding=1),   # [batch, 3, 32, 32]\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded, encoded.view(-1, 48*4*4)\n",
    "\n",
    "# class ResizeConv2d(nn.Module):\n",
    "\n",
    "#     def __init__(self, in_channels, out_channels, kernel_size, scale_factor, mode='nearest'):\n",
    "#         super().__init__()\n",
    "#         self.scale_factor = scale_factor\n",
    "#         self.mode = mode\n",
    "#         self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.interpolate(x, scale_factor=self.scale_factor, mode=self.mode)\n",
    "#         x = self.conv(x)\n",
    "#         return x\n",
    "\n",
    "# class BasicBlockEnc(nn.Module):\n",
    "\n",
    "#     def __init__(self, in_planes, stride=1):\n",
    "#         super().__init__()\n",
    "\n",
    "#         planes = in_planes*stride\n",
    "\n",
    "#         self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "#         self.bn1 = nn.BatchNorm2d(planes)\n",
    "#         self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#         self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "#         if stride == 1:\n",
    "#             self.shortcut = nn.Sequential()\n",
    "#         else:\n",
    "#             self.shortcut = nn.Sequential(\n",
    "#                 nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "#                 nn.BatchNorm2d(planes)\n",
    "#             )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = torch.relu(self.bn1(self.conv1(x)))\n",
    "#         out = self.bn2(self.conv2(out))\n",
    "#         out += self.shortcut(x)\n",
    "#         out = torch.relu(out)\n",
    "#         return out\n",
    "\n",
    "# class BasicBlockDec(nn.Module):\n",
    "\n",
    "#     def __init__(self, in_planes, stride=1):\n",
    "#         super().__init__()\n",
    "\n",
    "#         planes = int(in_planes/stride)\n",
    "\n",
    "#         self.conv2 = nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#         self.bn2 = nn.BatchNorm2d(in_planes)\n",
    "#         # self.bn1 could have been placed here, but that messes up the order of the layers when printing the class\n",
    "\n",
    "#         if stride == 1:\n",
    "#             self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#             self.bn1 = nn.BatchNorm2d(planes)\n",
    "#             self.shortcut = nn.Sequential()\n",
    "#         else:\n",
    "#             self.conv1 = ResizeConv2d(in_planes, planes, kernel_size=3, scale_factor=stride)\n",
    "#             self.bn1 = nn.BatchNorm2d(planes)\n",
    "#             self.shortcut = nn.Sequential(\n",
    "#                 ResizeConv2d(in_planes, planes, kernel_size=3, scale_factor=stride),\n",
    "#                 nn.BatchNorm2d(planes)\n",
    "#             )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = torch.relu(self.bn2(self.conv2(x)))\n",
    "#         out = self.bn1(self.conv1(out))\n",
    "#         out += self.shortcut(x)\n",
    "#         out = torch.relu(out)\n",
    "#         return out\n",
    "\n",
    "# class ResNet9Enc(nn.Module):\n",
    "\n",
    "#     def __init__(self, num_Blocks=[1,1,1,1], z_dim=10, nc=3):\n",
    "#         super().__init__()\n",
    "#         self.in_planes = 12\n",
    "#         self.z_dim = z_dim\n",
    "#         self.conv1 = nn.Conv2d(nc, 12, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "#         self.bn1 = nn.BatchNorm2d(12)\n",
    "#         self.layer1 = self._make_layer(BasicBlockEnc, 24, num_Blocks[0], stride=2)\n",
    "#         self.layer2 = self._make_layer(BasicBlockEnc, 48, num_Blocks[1], stride=2)\n",
    "\n",
    "#     def _make_layer(self, BasicBlockEnc, planes, num_Blocks, stride):\n",
    "#         strides = [stride] + [1]*(num_Blocks-1)\n",
    "#         layers = []\n",
    "#         for stride in strides:\n",
    "#             layers += [BasicBlockEnc(self.in_planes, stride)]\n",
    "#             self.in_planes = planes\n",
    "#         return nn.Sequential(*layers)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = torch.relu(self.bn1(self.conv1(x)))\n",
    "#         x = self.layer1(x)\n",
    "#         x = self.layer2(x)\n",
    "#         x = x.view(-1, 48*4*4)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "\n",
    "#         return x\n",
    "\n",
    "# class ResNet9Dec(nn.Module):\n",
    "\n",
    "#     def __init__(self, num_Blocks=[1,1,1,1], z_dim=10, nc=3):\n",
    "#         super().__init__()\n",
    "#         self.in_planes = 48\n",
    "\n",
    "#         self.layer2 = self._make_layer(BasicBlockDec, 24, num_Blocks[1], stride=2)\n",
    "#         self.layer1 = self._make_layer(BasicBlockDec, 12, num_Blocks[0], stride=2)\n",
    "#         self.conv1 = ResizeConv2d(12, nc, kernel_size=3, scale_factor=1)\n",
    "\n",
    "#     def _make_layer(self, BasicBlockDec, planes, num_Blocks, stride):\n",
    "#         strides = [stride] + [1]*(num_Blocks-1)\n",
    "#         layers = []\n",
    "#         for stride in reversed(strides):\n",
    "#             layers += [BasicBlockDec(self.in_planes, stride)]\n",
    "#         self.in_planes = planes\n",
    "#         return nn.Sequential(*layers)\n",
    "\n",
    "#     def forward(self, z):\n",
    "#         x = z.view(z.size(0), 768, 1, 1)\n",
    "#         x = F.interpolate(x, scale_factor=2)\n",
    "#         x = x.reshape((z.size(0), 48, 8, 8))\n",
    "#         x = self.layer2(x)\n",
    "#         x = self.layer1(x)\n",
    "#         x = torch.sigmoid(self.conv1(x))\n",
    "#         x = x.view(x.size(0), 3, 32, 32)\n",
    "#         return x\n",
    "\n",
    "# class Autoencoder(nn.Module):\n",
    "\n",
    "#     def __init__(self, z_dim):\n",
    "#         super().__init__()\n",
    "#         self.encoder = ResNet9Enc(z_dim=z_dim)\n",
    "#         self.decoder = ResNet9Dec(z_dim=z_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         z = self.encoder(x)\n",
    "#         x = self.decoder(z)\n",
    "#         return x, z\n",
    "\n",
    "def test():\n",
    "    net = Autoencoder()\n",
    "    net = net.to(device)\n",
    "    item = iter(train_loader)\n",
    "    img, _ = next(item)\n",
    "\n",
    "    img = img[0]\n",
    "    img = img[None, :]\n",
    "    img = img.to(device)\n",
    "    img_cons, rep  = net(img)\n",
    "    std_t = torch.tensor(std).to(device)\n",
    "    mean_t = torch.tensor(mean).to(device)\n",
    "    img_cons = img_cons * std_t[:, None, None] + mean_t[:, None, None]\n",
    "    img = img * std_t[:, None, None] + mean_t[:, None, None]\n",
    "    # print(rep)\n",
    "    print(rep.shape, \"  \", img_cons.shape)\n",
    "    temp_r = imshow(img.cpu()[0])\n",
    "    imshow(img_cons.detach().cpu()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 551
    },
    "id": "eg8mUT4q9B3K",
    "outputId": "b0c85e30-63c7-42a8-b59d-add4ecb84551"
   },
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DRnX01EI3jtY"
   },
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    'train': train_loader,\n",
    "    'val': test_loader,\n",
    "    'test': test_loader\n",
    "}\n",
    "dataset_sizes = {\n",
    "    'train': len(train_loader.dataset),\n",
    "    'val': len(test_loader.dataset),\n",
    "    'test': len(test_loader.dataset),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6yEELkFx48FJ"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def sparse_loss(autoencoder, images):\n",
    "    loss = 0\n",
    "    values = images\n",
    "    model_children = list(autoencoder.children())\n",
    "    for i in range(len(model_children)):\n",
    "        values = F.relu((model_children[i](values)))\n",
    "        loss += torch.mean(torch.abs(values))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vCBJSUK3CKPj"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "from torch import autograd\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train','val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, _ in tqdm(dataloaders[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    reconstructed_img, hidden_rep = model(inputs)\n",
    "                    \n",
    "                    # l1_loss = sparse_loss(model, inputs) * 0.001\n",
    "                    loss = criterion(reconstructed_img, inputs)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        with autograd.detect_anomaly():\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "#                         for name, param in model_ft.named_parameters():\n",
    "#                             print(name, param.grad.norm())\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss}')          \n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "#             if phase == 'val':\n",
    "                print('Saving..')\n",
    "                state = {\n",
    "                    'model': model.state_dict(),\n",
    "                    'loss': epoch_loss,\n",
    "                    'epoch': epoch,\n",
    "                }\n",
    "                # if not os.path.isdir('checkpoint'):\n",
    "                #     os.mkdir('checkpoint')\n",
    "                torch.save(state, './tiny_ae_layers5.pth')\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val loss: {best_loss:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XAlqmmI5CLPY"
   },
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "model_ft = Autoencoder()\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "exp_lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer_ft, milestones=[60, 120, 160], gamma=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 919
    },
    "id": "DHFjCKEED41l",
    "outputId": "03fe51d0-cf0e-4243-ae9f-0b19fe479eed",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model_ft.named_parameters():\n",
    "    if param.grad != None:\n",
    "        print(name, param.grad.norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 533
    },
    "id": "GH5IE1oRD5HV",
    "outputId": "0b04bd72-b849-45cc-fc6d-8eb136d30f01",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "device = 'cuda'\n",
    "model_ft.eval()\n",
    "item = iter(train_loader)\n",
    "img, _ = next(item)\n",
    "\n",
    "img = img[0]\n",
    "img = img[None, :]\n",
    "img = img.to(device)\n",
    "img_cons, rep  = model_ft(img)\n",
    "std_t = torch.tensor(std).to(device)\n",
    "mean_t = torch.tensor(mean).to(device)\n",
    "img_cons = img_cons * std_t[:, None, None] + mean_t[:, None, None]\n",
    "img = img * std_t[:, None, None] + mean_t[:, None, None]\n",
    "print(rep.shape, \"  \", img_cons.shape)\n",
    "temp_r = imshow(img.cpu()[0])\n",
    "imshow(img_cons.detach().cpu()[0])\n",
    "print(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IY9N1bRbifOH"
   },
   "outputs": [],
   "source": [
    "train_image_embeddings = []\n",
    "train_image_labels = []\n",
    "\n",
    "def populate_embedding_lable_list(model):\n",
    "    global train_image_embeddings, train_image_labels\n",
    "\n",
    "    model.eval()   # Set model to evaluate mode\n",
    "    # Iterate over data.\n",
    "    for inputs, labels in tqdm(dataloaders['train']):\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        # forward\n",
    "        # track history if only in train\n",
    "        with torch.set_grad_enabled(False):\n",
    "            reconstructed_img, hidden_rep = model(inputs)\n",
    "            train_image_embeddings.extend(hidden_rep)\n",
    "            train_image_labels.extend(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "populate_embedding_lable_list(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IvT1gFwrkJHW"
   },
   "outputs": [],
   "source": [
    "train_image_labels = torch.flatten(torch.tensor(train_image_labels))\n",
    "# torch.save(train_image_labels, '/content/drive/MyDrive/TDML_Project/train_image_labels.pt')\n",
    "\n",
    "train_image_embeddings_tensor = torch.stack(train_image_embeddings)\n",
    "# torch.save(train_image_embeddings_tensor, '/content/drive/MyDrive/TDML_Project/train_image_embeddings_tensor.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7NW3z-R3j1ys",
    "outputId": "3dbfd612-238f-41b2-94c8-bef5dcbf3adc"
   },
   "outputs": [],
   "source": [
    "print(len(sorted(set(list(train_loader.dataset.targets)))))\n",
    "print(len(sorted(set(list(train_image_labels.numpy())))))\n",
    "print(train_image_labels.shape, \" \", train_image_embeddings_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xYTauHBYoCOY",
    "outputId": "2af54ca8-cc55-4d3d-a07f-02d92b4f6e36"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def find_indices(list_to_check, item_to_find):\n",
    "    return [idx for idx, value in enumerate(list_to_check) if value == item_to_find]\n",
    "  \n",
    "image_label_list = list(train_image_labels.numpy())\n",
    "classes_to_index = {i: [] for i in range(100)}\n",
    "for key in classes_to_index.keys():\n",
    "    indexes = find_indices(image_label_list, key)\n",
    "    random_40 = random.sample(indexes,40)\n",
    "    classes_to_index[key] = random_40\n",
    "\n",
    "print(classes_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oMiTLnowb7WC",
    "outputId": "b2710e90-3d85-4ba1-f65d-70b546e6d2c3"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "final_indices = list(classes_to_index.values())\n",
    "final_indices = sorted([item for sublist in final_indices for item in sublist])\n",
    "print(final_indices)\n",
    "print(len(final_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bDlWA_BslCAi"
   },
   "outputs": [],
   "source": [
    "similarity_matrix = np.zeros((100,100,2))\n",
    "# device = 'cpu'\n",
    "# print(similarity_matrix[0])\n",
    "# print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MLMsbiEkpaVg",
    "outputId": "2afd9dfe-9702-490c-d891-7bce3f78d0e6"
   },
   "outputs": [],
   "source": [
    "cosine_sim = nn.CosineSimilarity(dim=0, eps=1e-8)\n",
    "print(train_image_embeddings[0].shape)\n",
    "cosine_sim(train_image_embeddings[2957], train_image_embeddings[20364].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FLTnqIWjxgPa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iyzbk3hwmKDC",
    "outputId": "df8c0bb7-486a-469f-fd15-373204862855"
   },
   "outputs": [],
   "source": [
    "for act_i in tqdm(range(len(final_indices))):\n",
    "    for act_j in range(act_i+1, len(final_indices)):\n",
    "        i, j = final_indices[act_i], final_indices[act_j]\n",
    "        class_i, class_j = train_image_labels[i], train_image_labels[j]\n",
    "        emb_i, emb_j = train_image_embeddings[i], train_image_embeddings[j]\n",
    "        sim_score = cosine_sim(emb_i, emb_j).detach().item()\n",
    "    \n",
    "        if class_i != class_j:\n",
    "            similarity_matrix[class_i][class_j][0] += sim_score\n",
    "            similarity_matrix[class_i][class_j][1] += 1\n",
    "\n",
    "            similarity_matrix[class_j][class_i][0] += sim_score\n",
    "            similarity_matrix[class_j][class_i][1] += 1\n",
    "        else:\n",
    "            similarity_matrix[class_i][class_i][0] += sim_score\n",
    "            similarity_matrix[class_i][class_i][1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LSsYul3mqfhP"
   },
   "outputs": [],
   "source": [
    "avg_sim = np.zeros((100,100))\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        if i == j:\n",
    "            avg_sim[i][j] = similarity_matrix[i][j][0]/similarity_matrix[i][j][1] + 0.0225\n",
    "        else:\n",
    "            avg_sim[i][j] = similarity_matrix[i][j][0]/similarity_matrix[i][j][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d7thC97I3elJ"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "x_distribution = F.softmax(torch.tensor(avg_sim), dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ENFkPbGpoZgs",
    "outputId": "f62a7630-7a64-4d30-ad30-27e38d4300e6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_distribution[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sim[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xjBn8ziSlcN_",
    "outputId": "89405c3f-af63-455c-9821-b09dfd0209eb"
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(len(x_distribution)):\n",
    "    if i == torch.argmax(x_distribution[i]):\n",
    "        count += 1\n",
    "        # print(i)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oYxvkwP4JdpS"
   },
   "outputs": [],
   "source": [
    "torch.save(avg_sim, './ae_similarity_mat.pt')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "32e893a1b4f24c2e9b7ace0db3715aec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f75f21085d34e3a8f8400fdcc663643": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4688d70dd6be4a3e90b45367149f5aac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "918329870f324a37b7c886307130e4f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f75f21085d34e3a8f8400fdcc663643",
      "placeholder": "​",
      "style": "IPY_MODEL_4688d70dd6be4a3e90b45367149f5aac",
      "value": "100%"
     }
    },
    "9957632d96884fd68bd949a3c374bc26": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_918329870f324a37b7c886307130e4f0",
       "IPY_MODEL_a58b59067c224f68b2224372f49c8c37",
       "IPY_MODEL_c0284820254443aa882594ff94508904"
      ],
      "layout": "IPY_MODEL_e338f0bd470a42f48478d564c8438c34"
     }
    },
    "a176b2769214468990fc93ec6f64d83c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a58b59067c224f68b2224372f49c8c37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_32e893a1b4f24c2e9b7ace0db3715aec",
      "max": 169001437,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d6118f6d47074f6385ef881220c3d0d8",
      "value": 169001437
     }
    },
    "c0284820254443aa882594ff94508904": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a176b2769214468990fc93ec6f64d83c",
      "placeholder": "​",
      "style": "IPY_MODEL_f8cb745f778a45ae8711f52ca3ca89ec",
      "value": " 169001437/169001437 [00:13&lt;00:00, 13685478.41it/s]"
     }
    },
    "d6118f6d47074f6385ef881220c3d0d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e338f0bd470a42f48478d564c8438c34": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f8cb745f778a45ae8711f52ca3ca89ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
